{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07332728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset, load_metric\n",
    "from transformers import (\n",
    "    MarianTokenizer,\n",
    "    MBart50TokenizerFast,\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    utils\n",
    ")\n",
    "from datasets import concatenate_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151f930",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6061ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Global Parametre\n",
    "utils.logging.set_verbosity(50)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "config = helper.read_config()\n",
    "SEED = 99\n",
    "TOKEN = config['Data_Tokens']['token']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "asian_tokens = r\"data/tokens.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963bfb1",
   "metadata": {},
   "source": [
    "### Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57323d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tk_model):\n",
    "    utils.logging.set_verbosity(50)\n",
    "    mr_tknzr = MarianTokenizer.from_pretrained(tk_model)\n",
    "    with open(asian_tokens) as fp:\n",
    "        tokens = fp.read().split(\"\\n\")\n",
    "    mr_tknzr.add_tokens(tokens, special_tokens=True)\n",
    "    return mr_tknzr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb158d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helsinki-NLP/opus-mt-en-zh\n"
     ]
    }
   ],
   "source": [
    "# Loading pre-trained tokenizer.\n",
    "utils.logging.set_verbosity(50)\n",
    "xxlang = 'zh'\n",
    "pretrained_models = {\n",
    "        \"zh\":\"Helsinki-NLP/opus-mt-en-zh\",\n",
    "        \"ja\":\"Helsinki-NLP/opus-tatoeba-en-ja\"\n",
    "    }\n",
    "\n",
    "print(pretrained_models[xxlang])\n",
    "model_pretrained = pretrained_models[xxlang]\n",
    "pretrained_tknzr = load_tokenizer(pretrained_models[xxlang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0530bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing\n",
    "ip_lenght = 128\n",
    "op_lenght = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb12b4f9",
   "metadata": {},
   "source": [
    "### Data Pre-Processing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc91c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(examples):\n",
    "    utils.logging.set_verbosity(50)\n",
    "    ip_sent = [s for s in examples[\"Sent_en\"]]\n",
    "    target_sent = [s for s in examples[\"Sent_yy\"]]\n",
    "    \n",
    "    model_ip = pretrained_tknzr(ip_sent, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    labels = pretrained_tknzr(target_sent, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    if len(examples['Sent_en']) > 1 and (len(model_ip['input_ids'][0]) != len(model_ip[\"input_ids\"][1])):\n",
    "        print (\"Error!\", )\n",
    "\n",
    "    model_ip[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d01a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"sacrebleu\")\n",
    "    \n",
    "def compute_bleu(eval_preds, tokenizer=pretrained_tknzr , metric= metric):\n",
    "    utils.logging.set_verbosity(50)\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels] \n",
    "        \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": round(result[\"score\"], 4)}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = round(np.mean(prediction_lens), 4)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988debc",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72012bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-1fc2412c1e98613e\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/data-1fc2412c1e98613e/0.0.0/1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63621b84272a4587b9fcfab78b532326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\"train\": \"train.parquet\", \"test\": \"test.parquet\", \"eval\": \"eval.parquet\"}\n",
    "dataset = load_dataset(\"/workspace/project/data\", data_files= data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff07c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "transformation_cols = ['input_ids', 'labels', 'attention_mask']\n",
    "yylang = 'ms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a183b109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['SID', 'Sent_en', 'Sent_yy', 'lang_yy'],\n",
       "        num_rows: 179199\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['SID', 'Sent_en', 'Sent_yy', 'lang_yy'],\n",
       "        num_rows: 10101\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['SID', 'Sent_en', 'Sent_yy', 'lang_yy'],\n",
       "        num_rows: 9930\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn(example):\n",
    "    return example['lang_yy'] == yylang\n",
    "\n",
    "# data_subset = dataset.filter(lambda example: example['lang_yy'] == yylang)\n",
    "\n",
    "data_subset = dataset.filter(filter_fn)\n",
    "\n",
    "print(10)\n",
    "# data_subset = data_subset.map(data_preproces, batched=True, batch_size=batch_size*3, fn_kwargs={'pretrained_tknzr': pretrained_tknzr})\n",
    "data_subset1 = data_subset.map(data_preprocess, batched=True, batch_size=batch_size*3)\n",
    "print(11)\n",
    "data_subset1.set_format(type='torch', columns=transformation_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fc179",
   "metadata": {},
   "source": [
    "### Fine Tuning the Pre-Trained Model on one target Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36440d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_pretrained).to(device)\n",
    "model.resize_token_embeddings(len(pretrained_tknzr))\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(pretrained_tknzr, model=model)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = f\"NMT_PFT_en-{xxlang}-to-{yylang}\",\n",
    "    seed = 99,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    log_level = 'warning',\n",
    "    disable_tqdm = False,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size= 16,\n",
    "    per_device_eval_batch_size= 16,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps = 10,\n",
    "    num_train_epochs=15,\n",
    "    predict_with_generate=True,\n",
    "    remove_unused_columns = True,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    save_strategy = \"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=data_subset[\"train\"],\n",
    "    eval_dataset=data_subset[\"eval\"],\n",
    "    tokenizer=pretrained_tknzr,\n",
    "    compute_metrics=compute_bleu,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9cf861",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c7701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442a81b",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(data_subset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4432c",
   "metadata": {},
   "source": [
    "# ----- Pure - Fine tunning -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a9c16",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98aa9efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function concat_data.<locals>.<lambda> at 0x7f801ed63310> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cf1ae0176c46caa7eda769348facc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opus100 (/root/.cache/huggingface/datasets/opus100/en-zh/0.0.0/256f3196b69901fb0c79810ef468e2c4ed84fbd563719920b1ff1fdc750f7704)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9011ef1114f24eaf81313d9dc0e1a561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Alt dataset used for mix training****\n",
      "Total Languages: 9\n",
      "Sentences: 161111\n",
      "****Opus dataset used for mix training****\n",
      "Languages: en-zh\n",
      "Sentences: Dataset({\n",
      "    features: ['Sent_en', 'Sent_yy'],\n",
      "    num_rows: 18000\n",
      "})\n",
      "****Final dataset for Mix Training****\n",
      "Languages: 10\n",
      "Sentences: 179111\n"
     ]
    }
   ],
   "source": [
    "yy_subset = ['fil', 'hi', 'id', 'ja', 'khm', 'ms', 'my', 'th', 'vi']\n",
    "xx_size = 18_000\n",
    "\n",
    "def rename_key(example):\n",
    "    example[\"Sent_en\"] = example[\"translation\"][\"en\"]\n",
    "    example[\"Sent_yy\"] = example[\"translation\"][xxlang]\n",
    "    return example\n",
    "    \n",
    "def concat_data(data, xxlang, xx_size = 20_000, mix_subset = [\"vi\"] ):\n",
    "    \n",
    "    select_lang = lambda x: x[\"lang_yy\"] in mix_subset\n",
    "    \n",
    "    data_subset = data[\"train\"].filter(select_lang).remove_columns([\"SID\", \"lang_yy\"])\n",
    "    \n",
    "    xx_data = load_dataset(\"opus100\", f'en-{xxlang}', split = \"train\")\n",
    "\n",
    "    xx_train = xx_data.select(range(xx_size)).map(rename_key).remove_columns(\"translation\")\n",
    "\n",
    "    data_concat = concatenate_datasets([data_subset, xx_train])\n",
    "    \n",
    "    pad = \"*\"*4\n",
    "    print(f\"{pad}Alt dataset used for mix training{pad}\\nTotal Languages: {len(mix_subset)}\\nSentences: {len(data_subset)}\")\n",
    "    print(f\"{pad}Opus dataset used for mix training{pad}\\nLanguages: en-zh\\nSentences: {xx_train}\")\n",
    "    print(f\"{pad}Final dataset for Mix Training{pad}\\nLanguages: {1+len(mix_subset)}\\nSentences: {len(data_concat)}\")\n",
    "    \n",
    "    return data_concat.shuffle(seed = SEED)\n",
    "\n",
    "data_mix = concat_data(data= dataset, xxlang = xxlang, xx_size = xx_size, mix_subset = ['fil', 'hi', 'id', 'ja', 'khm', 'ms', 'my', 'th', 'vi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d4c60",
   "metadata": {},
   "source": [
    "### Pre- Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b830e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2ebb5440654598a04c3d11aa93afe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/933 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b328b45209c44912b44055ba16c22bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b14b615f9d3404c83f2ad4f24062437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_mix_train = data_mix.map(data_preproces, batched=True, batch_size=batch_size*3, fn_kwargs={'pretrained_tknzr': pretrained_tknzr})\n",
    "data_mix_train = data_mix.map(data_preprocess, batched=True, batch_size=batch_size*3)\n",
    "data_mix_train.set_format(type='torch', columns=transformation_cols)\n",
    "\n",
    "eval_data_mix = dataset[\"eval\"].filter(lambda x: x[\"lang_yy\"] in yy_subset)\n",
    "# eval_data_mix = eval_data_mix.map(data_preproces, batched=True, batch_size=batch_size, fn_kwargs={'pretrained_tknzr': pretrained_tknzr})\n",
    "eval_data_mix = eval_data_mix.map(data_preprocess, batched=True, batch_size=batch_size)\n",
    "eval_data_mix.set_format(type='torch', columns=transformation_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5add4d9",
   "metadata": {},
   "source": [
    "### Load Model and Train on Mix Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d851c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.logging.set_verbosity(25)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_pretrained).to(device)\n",
    "model.resize_token_embeddings(len(pretrained_tknzr))\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(pretrained_tknzr, model=model)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = f\"NMT_Mix_en-{xxlang}-to-{yylang}\",\n",
    "    seed = 99,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 10_000,\n",
    "    log_level = 'warning',\n",
    "    disable_tqdm = False,\n",
    "    \n",
    "    save_total_limit=3,\n",
    "    save_strategy = 'steps', #\"epoch\",\n",
    "    save_steps = 10_000,\n",
    "    push_to_hub = False,\n",
    "    hub_token = TOKEN,\n",
    "    hub_model_id = f\"NMT_Mix_en-{xxlang}-to-{yylang}\",\n",
    "    hub_strategy = \"checkpoint\",\n",
    "    \n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps = 10,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    remove_unused_columns = True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=data_mix_train,\n",
    "    eval_dataset=eval_data_mix,\n",
    "    data_collator=collator,\n",
    "    tokenizer=pretrained_tknzr,\n",
    "    compute_metrics=compute_bleu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e3305ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='111945' max='111945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111945/111945 6:27:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.250600</td>\n",
       "      <td>1.321967</td>\n",
       "      <td>9.938800</td>\n",
       "      <td>58.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>1.021635</td>\n",
       "      <td>17.274000</td>\n",
       "      <td>45.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.898940</td>\n",
       "      <td>21.253100</td>\n",
       "      <td>45.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.743900</td>\n",
       "      <td>0.822815</td>\n",
       "      <td>23.405600</td>\n",
       "      <td>43.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.655400</td>\n",
       "      <td>0.775739</td>\n",
       "      <td>25.612600</td>\n",
       "      <td>44.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.620200</td>\n",
       "      <td>0.739401</td>\n",
       "      <td>26.969800</td>\n",
       "      <td>44.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.547200</td>\n",
       "      <td>0.715244</td>\n",
       "      <td>27.851600</td>\n",
       "      <td>43.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.693762</td>\n",
       "      <td>28.675900</td>\n",
       "      <td>44.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.479800</td>\n",
       "      <td>0.679192</td>\n",
       "      <td>29.552000</td>\n",
       "      <td>43.759900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.667674</td>\n",
       "      <td>30.088100</td>\n",
       "      <td>44.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.658994</td>\n",
       "      <td>30.423800</td>\n",
       "      <td>44.393400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=111945, training_loss=0.7493315314590608, metrics={'train_runtime': 23243.1397, 'train_samples_per_second': 38.53, 'train_steps_per_second': 4.816, 'total_flos': 6.071570040619008e+16, 'train_loss': 0.7493315314590608, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e08e3",
   "metadata": {},
   "source": [
    "### Loding the model trained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0c8e011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(101462, 512, padding_idx=65000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(f\"NMT_Mix_en-{xxlang}-to-{yylang}\")\n",
    "model.resize_token_embeddings(len(pretrained_tknzr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1e04e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3fbd49d0c047659e96c922f9a5a710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad70c394f4c45559a07f2386a5a5a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5e64c2124d4d5c92551ceef22c2178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d1de5d10134c96881e2e346a9fce3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ced8b3666845ba8ab16599712eafb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045aba8a3f9147b6abcc9cfda16db398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_lang = 'ms'# == yylang\n",
    "\n",
    "data_subset = dataset.filter(lambda x: x[\"lang_yy\"] == target_lang)\n",
    "data_subset = data_subset.map(data_preprocess, batched=True, batch_size=batch_size*3)\n",
    "data_subset.set_format(type='torch', columns=transformation_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e99ead",
   "metadata": {},
   "source": [
    "### Load Model and Train on Target Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f18039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(pretrained_tknzr, model=model)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = f\"NMT_Final_{target_lang}\",\n",
    "    seed = 99,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    log_level = 'warning',\n",
    "    disable_tqdm = False,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    remove_unused_columns = True,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    save_strategy = \"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=data_subset[\"train\"],\n",
    "    eval_dataset=data_subset[\"eval\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer= pretrained_tknzr,\n",
    "    compute_metrics= compute_bleu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a833b4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11220' max='11220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11220/11220 29:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.426728</td>\n",
       "      <td>43.845800</td>\n",
       "      <td>37.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>0.419369</td>\n",
       "      <td>44.882200</td>\n",
       "      <td>37.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0.414721</td>\n",
       "      <td>45.725700</td>\n",
       "      <td>37.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.419384</td>\n",
       "      <td>46.302000</td>\n",
       "      <td>37.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.421675</td>\n",
       "      <td>46.766200</td>\n",
       "      <td>37.532700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1364: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11220, training_loss=0.19844589114401642, metrics={'train_runtime': 1796.476, 'train_samples_per_second': 49.956, 'train_steps_per_second': 6.246, 'total_flos': 6084417520926720.0, 'train_loss': 0.19844589114401642, 'epoch': 5.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eed5eff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[65000, 74604, 79074, ..., 65000, 65000, 65000],\n",
       "       [65000, 66619,     8, ..., 65000, 65000, 65000],\n",
       "       [65000, 74604, 80960, ..., 65000, 65000, 65000],\n",
       "       ...,\n",
       "       [65000, 70610,    13, ..., 65000, 65000, 65000],\n",
       "       [65000, 79469, 79178, ..., 65000, 65000, 65000],\n",
       "       [65000, 79480,    13, ..., 65000, 65000, 65000]]), label_ids=array([[80584, 70261, 79475, ..., 65000, 65000, 65000],\n",
       "       [66619,     8, 10139, ..., 65000, 65000, 65000],\n",
       "       [ 9280, 80347,     8, ..., 65000, 65000, 65000],\n",
       "       ...,\n",
       "       [79402,     8,  3005, ..., 65000, 65000, 65000],\n",
       "       [79469, 79157, 73645, ..., 65000, 65000, 65000],\n",
       "       [79480, 70161, 79433, ..., 65000, 65000, 65000]]), metrics={'eval_loss': 0.4476158618927002, 'eval_bleu': 45.1078, 'eval_gen_len': 38.0305, 'eval_runtime': 78.84, 'eval_samples_per_second': 12.9, 'eval_steps_per_second': 1.624})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(data_subset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e278e",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b0274f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 475.625x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 475.625x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu_df = pd.DataFrame(data={'BLEU Scores': ['Baseline BLEU','Fine-tuned BlEU']*7, \\\n",
    "    'Languages': ['Filipino', 'Filipino', 'Hindi', 'Hindi', 'Indonesian', \\\n",
    "        'Indonesian', 'Malay', 'Malay', 'Burmese', 'Burmese', 'Thai', 'Thai',\\\n",
    "            'Vietnamese', 'Vietnamese'], \\\n",
    "                'BLEU Score Zh': [24.45, 31.81, 30.49, 32.43, 26.41, 29.47,\\\n",
    "                    31.65, 33.35, 19.82, 26.81, 20.42, 23.91, 32.14, 36.91],\\\n",
    "                        'BLEU Score Ja': [17.62, 28.37, 17.96, 29.63, 26.33, 29.54,\\\n",
    "                            31.74, 39.88, 4.84, 25.11, 17.21, 21.42, 25.87, 33.45]\\\n",
    "                                })\n",
    "\n",
    "\n",
    "sns.catplot(x = 'Languages', y='BLEU Score Zh', hue = 'BLEU Scores',data=bleu_df, kind='bar')\n",
    "sns.catplot(x = 'Languages', y='BLEU Score Ja', hue = 'BLEU Scores',data=bleu_df, kind='bar')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469921d",
   "metadata": {},
   "source": [
    "As helper languages, Chinese and Japanese, the BLEU ratings for each configuration are displayed above. For the mix fine tune with pure tuning setups, regardless of the external parallel corpus for en-xx, the three-stage fine-tuned model, Pure-fine tuning with Chinese as a helper language achieved the highest BLEU scores for all seven target languages. Results for Baseline techniques using Japanese as the helper language demonstrate that NMT systems trained on 18k parallel phrases can only get a subpar outcome for the target language \"My\" while the other target languages achieve reasonably high BLEU scores (20).  No matter how it was applied, the combination of mixed pre-training and pure fine-tuning, as well as the addition of a sizeable external en-xx parallel corpus, consistently and dramatically improved the translation quality for each of the seven target languages. For all of the examples, pure fine-tuning using Chinese and Japanese as auxiliary languages produced the greatest results. The BLEU increased from 1.94 (Hi) to 6.99 (Ms) points when using the En-Zh corpus, and from 3.17 (Id) to 20.27 (My) points when using the En-Ja corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6abb62f",
   "metadata": {},
   "source": [
    "With improvements over the best scores of 1.94 (Hi) to 6.99 (Ms) BLEU points with the En-Zh corpus, the three-stage fine-tuned model using Chinese as a helper language consistently beat the En-Ja pure fine-tuned model. These models don't add any extra source sentences compared to baseline, as was previously mentioned. Therefore, we can claim with certainty that the benefit is the result of multilingualism. When all En-yy language pairings were utilised for the three-stage fine-tuning model, the En-Ja corpus likewise showed a significant superiority gain (for Hi, Ms, My, and Vi) over the En-Zh corpus. When all of the target languages were used, however, the En-Zh corpus frequently helped to achieve considerably better results than the En-Ja corpus (for Fi and Id). The size of the corpus, the similarity or overlap between the interest and helping target languages, and the proximity of the domain are three potential contributing variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf5c96",
   "metadata": {},
   "source": [
    "En-Ja corpus performed 2.48 BLEU points better than En-Zh corpus when used as helping data for En-Ja translation.\n",
    "The significant BLEU gap could be explained by the fact that the En-Ja corpus is more than twice as big as the En-Zh corpus. Even if it is conceivable that the Chinese and Japanese languages' vocabulary overlaps to aid in En-Ja translation, other languages also do better even when there isn't. We believe that translation into a low-resource target language may benefit from being the target side of the supporting corpus, but multistage fine-tuning does not require this overlap and instead performs best when it makes use of multilingualism during stage-wise tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
